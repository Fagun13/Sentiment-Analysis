{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "#https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "spl_characters = ['~','`','!','@','^','$','%','&','*','(',')','+','=','{','}','[',']',';',':','|','\\\\','\"',\"'\",'\\n','<','>',',','.','?','/','-','*','#']\n",
    "\n",
    "f = open(\"imdb_labelled.txt\", \"r\")\n",
    "s_w = set(stopwords.words('english'))\n",
    "\n",
    "strarr=[]\n",
    "for line in f:\n",
    "    arr=line.split(\"\\t\")\n",
    "    string=arr[0]\n",
    "    label=arr[1][:-1]\n",
    "    for k in spl_characters:\n",
    "        string=string.replace(k,' ')\n",
    "    \n",
    "    string=string.lower()\n",
    "    obj={'str':string,'label':int(label)}\n",
    "    strarr.append(obj)\n",
    "\n",
    "for obj in strarr:\n",
    "    w_t=word_tokenize(obj['str'])\n",
    "    fs = [w for w in w_t if not w in s_w]\n",
    "    a=\" \".join(fs)\n",
    "    obj['str']=a\n",
    "# train test dev ratio 60 20 20\n",
    "full_len=len(strarr)\n",
    "len_train=int(len(strarr)*0.6)\n",
    "len_dev=int(len(strarr)*0.2)\n",
    "train=strarr[0:len_train]\n",
    "dev=strarr[len_train:len_train+len_dev]\n",
    "test=strarr[len_train+len_dev:full_len]\n",
    "print(len(train)+len(test)+len(dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "trListAll=[]\n",
    "trListPos=[]\n",
    "trListNeg=[]\n",
    "trSetAll=set()\n",
    "trSetPos=set()\n",
    "trSetNeg=set()\n",
    "posDocCount=0\n",
    "negDocCount=0\n",
    "trSetAllCount={}\n",
    "trSetPosCount={}\n",
    "trSetNegCount={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data):\n",
    "    global trListAll,trListPos,trListNeg,trSetAll,trSetPos,trSetNeg,posDocCount,negDocCount,trSetAllCount,trSetPosCount    \n",
    "    for t in data:\n",
    "        arr=word_tokenize(t['str'])\n",
    "        trListAll.extend(arr)\n",
    "        trSetAll.update(arr)\n",
    "        if(t['label']==1):\n",
    "            trListPos.extend(arr)\n",
    "            trSetPos.update(arr)\n",
    "            posDocCount+=1\n",
    "        else:\n",
    "            trListNeg.extend(arr)\n",
    "            trSetNeg.update(arr)\n",
    "            negDocCount+=1\n",
    "            \n",
    "    #count for each word in all document process        \n",
    "    for twr in trSetAll:\n",
    "        count=0\n",
    "        for t in data:\n",
    "            if twr in t['str']:\n",
    "                count+=1\n",
    "        trSetAllCount[twr]=count\n",
    "        \n",
    "    for key in trSetAllCount:\n",
    "        #print(key,\" \",trSetAllCount[key])\n",
    "        trSetAllCount[key]=trSetAllCount[key]/len(train)\n",
    "        \n",
    "    #count for each word in pos document process\n",
    "\n",
    "    for twr in trSetPos:\n",
    "        count=0\n",
    "        for t in data:\n",
    "            if(t['label']==1):\n",
    "                if twr in t['str']:\n",
    "                    count+=1\n",
    "        trSetPosCount[twr]=count\n",
    "        \n",
    "     #count for each word in neg document process   \n",
    "    for twr in trSetNeg:\n",
    "        count=0\n",
    "        for t in data:\n",
    "            if(t['label']==0):\n",
    "                if twr in t['str']:\n",
    "                    count+=1\n",
    "        trSetNegCount[twr]=count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(data):\n",
    "    acc_count=0\n",
    "    global posDocCount,negDocCount\n",
    "    pos_class_prob=posDocCount/len(train)\n",
    "    neg_class_prob=negDocCount/len(train)\n",
    "    for d in data:\n",
    "        w_t_arr=word_tokenize(d['str'])\n",
    "        pos_prob=1\n",
    "        neg_prob=1\n",
    "        final=-1\n",
    "        for w in w_t_arr:\n",
    "            if(w not in trSetPosCount):\n",
    "                pos_prob=pos_prob*0\n",
    "            else:\n",
    "                prob=trSetPosCount[w]/posDocCount\n",
    "                pos_prob=pos_prob*prob\n",
    "            if(w not in trSetNegCount):\n",
    "                neg_prob=neg_prob*0\n",
    "            else:\n",
    "                prob=trSetNegCount[w]/negDocCount\n",
    "                neg_prob=neg_prob*prob     \n",
    "        pos_prob=pos_prob*pos_class_prob\n",
    "        neg_prob=neg_prob*neg_class_prob\n",
    "        if(pos_prob>neg_prob):\n",
    "            final=1\n",
    "        else:\n",
    "            final=0\n",
    "        if(final==d['label']):\n",
    "            acc_count=acc_count+1\n",
    "    acc_count=acc_count*100/len(data) \n",
    "    return acc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy  98.33333333333333  test accuracy  49.0\n"
     ]
    }
   ],
   "source": [
    "train_model(train)\n",
    "train_acc=test_data(train)\n",
    "test_acc=test_data(test)\n",
    "print(\"train accuracy \",train_acc,\" test accuracy \",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold accuracy list [50.0, 25.0, 65.0, 37.5, 22.5]\n",
      "5-fold accuracy  40.0\n"
     ]
    }
   ],
   "source": [
    "#5-fold cross validation\n",
    "#https://stackoverflow.com/questions/43968369/subtracting-one-list-from-another-in-python\n",
    "lendev=int(len(dev)/5)\n",
    "accuracy_list=[]\n",
    "for i in range(5):\n",
    "    acc=0\n",
    "    testdata=dev[i*lendev:(i+1)*lendev]\n",
    "    #testdata=dev-traindata\n",
    "    traindata= [x for x in dev if x not in testdata]\n",
    "    train_model(traindata)\n",
    "    acc=test_data(testdata)\n",
    "    accuracy_list.append(acc)\n",
    "print(\"5-fold accuracy list\",accuracy_list)   \n",
    "\n",
    "final_acc=0\n",
    "for a in accuracy_list:\n",
    "    final_acc=final_acc+a\n",
    "final_acc=final_acc/len(accuracy_list)\n",
    "print(\"5-fold accuracy \",final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev accuracy without 5-fold  43.5\n"
     ]
    }
   ],
   "source": [
    "trListAll=[]\n",
    "trListPos=[]\n",
    "trListNeg=[]\n",
    "trSetAll=set()\n",
    "trSetPos=set()\n",
    "trSetNeg=set()\n",
    "posDocCount=0\n",
    "negDocCount=0\n",
    "trSetAllCount={}\n",
    "trSetPosCount={}\n",
    "trSetNegCount={}\n",
    "train_model(train)\n",
    "acc=test_data(dev)\n",
    "print(\"Dev accuracy without 5-fold \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc_laplas(data,alpha):\n",
    "    acc_count=0\n",
    "    global posDocCount,negDocCount\n",
    "    pos_class_prob=posDocCount/len(train)\n",
    "    neg_class_prob=negDocCount/len(train)\n",
    "    for d in data:\n",
    "        w_t_arr=word_tokenize(d['str'])\n",
    "        pos_prob=1\n",
    "        neg_prob=1\n",
    "        final=-1\n",
    "        for w in w_t_arr:\n",
    "            if(w not in trSetPosCount):\n",
    "                pos_prob=pos_prob*alpha/(posDocCount+(2*alpha))\n",
    "            else:\n",
    "                prob=trSetPosCount[w]+alpha/(posDocCount+(2*alpha))\n",
    "                pos_prob=pos_prob*prob\n",
    "            if(w not in trSetNegCount):\n",
    "                neg_prob=neg_prob*alpha/(negDocCount+(2*alpha))\n",
    "            else:\n",
    "                prob=trSetNegCount[w]+alpha/(negDocCount+(2*alpha))\n",
    "                neg_prob=neg_prob*prob\n",
    "        pos_prob=pos_prob*pos_class_prob\n",
    "        neg_prob=neg_prob*neg_class_prob\n",
    "        if(pos_prob>neg_prob):\n",
    "            final=1\n",
    "        else:\n",
    "            final=0\n",
    "        if(final==d['label']):\n",
    "            acc_count=acc_count+1\n",
    "    acc_count=acc_count*100/len(data) \n",
    "    return acc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.5\n",
      "69.5\n",
      "69.5\n",
      "69.5\n",
      "69.5\n",
      "69.0\n",
      "71.5\n"
     ]
    }
   ],
   "source": [
    "acc1=cal_acc_laplas(dev,1)\n",
    "print(acc1)\n",
    "acc2=cal_acc_laplas(dev,0.1)\n",
    "print(acc2)\n",
    "acc3=cal_acc_laplas(dev,0.01)\n",
    "print(acc3)\n",
    "acc4=cal_acc_laplas(dev,0.001)\n",
    "print(acc4)\n",
    "acc5=cal_acc_laplas(dev,0.0001)\n",
    "print(acc5)\n",
    "acc6=cal_acc_laplas(dev,10)\n",
    "print(acc6)\n",
    "acc7=cal_acc_laplas(dev,100)\n",
    "print(acc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 10 positive and negative words with highest probability.\n",
    "p_p={}\n",
    "n_p={}\n",
    "for key in trSetPosCount:\n",
    "    p_p[key]=trSetPosCount[key]/trSetAllCount[key]\n",
    "for key in trSetNegCount:\n",
    "    n_p[key]=trSetNegCount[key]/trSetAllCount[key]\n",
    "# here trSetAllCount is probability of each word.divide positive probability of word with probability of word as whole\n",
    "for key in p_p:\n",
    "    p_p[key]=p_p[key]/trSetAllCount[key]\n",
    "for key in n_p:\n",
    "    n_p[key]=n_p[key]/trSetAllCount[key]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive word  dict_keys(['room', 'drive', 'charming', 'local', 'ladies', 'sells', 'terrific', 'impressed', 'brilliant', 'coal'])\n",
      "Negative word  dict_keys(['reality', 'became', 'donlevy', 'insipid', 'reverse', 'contract', 'seemed', 'planned', 'cost', 'completed'])\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "#https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value \n",
    "#for sorting dictionary by value \n",
    "sorted_p_p = dict(sorted(p_p.items(), key=operator.itemgetter(1))[-10:])\n",
    "print(\"Positive word \",sorted_p_p.keys())\n",
    "sorted_n_p = dict(sorted(n_p.items(), key=operator.itemgetter(1))[-10:])\n",
    "print(\"Negative word \",sorted_n_p.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.5\n"
     ]
    }
   ],
   "source": [
    "testfinacc=cal_acc_laplas(test,100)\n",
    "print(testfinacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
